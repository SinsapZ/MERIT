# ğŸ¯ ä¸ºä»€ä¹ˆMERITä¸åº”è¯¥ä½¿ç”¨GNN

## ğŸ“Š å®éªŒè¯æ®

### ä¸‰è½®å®Œæ•´å®éªŒå¯¹æ¯”

| é…ç½® | Test Acc | Test F1 | æ ‡å‡†å·® | vs MedGNN | è¯´æ˜ |
|------|----------|---------|--------|-----------|------|
| **MERIT (æ— GNN)** | **78.00%** | **74.15%** | 2.46% | -4.6% | âœ… **æœ€ä½³** |
| MERIT (æœ‰GNN, 10 ep) | 74.51% | 68.80% | 4.46% | -8.09% | âŒ è®­ç»ƒä¸è¶³ |
| MERIT (æœ‰GNN, 100 ep) | 75.33% | 70.07% | 4.31% | -7.27% | âŒ ä»ç„¶æ›´å·® |
| **MedGNN (åŸºçº¿)** | **82.60%** | **80.25%** | 0.35% | - | ç›®æ ‡ |

**ç»“è®º**: 
- ğŸ”´ **GNNä½¿æ€§èƒ½ä¸‹é™ 2.67%** (78.00% â†’ 75.33%)
- ğŸ”´ **æ–¹å·®å¢åŠ  1.85%** (2.46% â†’ 4.31%)
- ğŸ”´ **è®­ç»ƒæ—¶é—´æ— å…³** (10æˆ–100 epochséƒ½ä¸è¡Œ)

---

## ğŸ”¬ ç†è®ºåˆ†æï¼šæ¶æ„å†²çª

### MedGNNçš„è®¾è®¡é€»è¾‘

```python
class MedGNN:
    def forward(self, x):
        # æ­¥éª¤1-3: ç‰¹å¾æå–
        features = [h1, h2, h3, h4]  # 4ä¸ªåˆ†è¾¨ç‡
        
        # æ­¥éª¤4: GNNèåˆå¤šåˆ†è¾¨ç‡
        gnn_out = self.mrgnn(features)
        # GNNå†…éƒ¨: å­¦ä¹ åˆ†è¾¨ç‡é—´ä¾èµ– â†’ Meanèåˆ
        # è¾“å‡º: single tensor (B, d_model, enc_in)
        
        # æ­¥éª¤5: ç®€å•åˆ†ç±»
        logits = self.linear(gnn_out)
        
        return logits
```

**GNNçš„ä½œç”¨**: èåˆå¤šåˆ†è¾¨ç‡ä¿¡æ¯åˆ°å•ä¸€è¡¨ç¤º

---

### MERITçš„è®¾è®¡é€»è¾‘

```python
class MERIT:
    def forward(self, x):
        # æ­¥éª¤1-3: ç‰¹å¾æå– (ä¸MedGNNç›¸åŒ)
        features = [h1, h2, h3, h4]  # 4ä¸ªåˆ†è¾¨ç‡
        
        # å¦‚æœä½¿ç”¨GNN:
        if self.use_gnn:
            features = self.mrgnn(features)
            # GNNæ··åˆäº†åˆ†è¾¨ç‡ä¿¡æ¯
            # ä½†è¾“å‡ºä»æ˜¯list: [h1', h2', h3', h4']
            # æ¯ä¸ªh_i'å·²ç»åŒ…å«äº†å…¶ä»–åˆ†è¾¨ç‡çš„ä¿¡æ¯
        
        # æ­¥éª¤4: è¯æ®èåˆ
        for i, h_i in enumerate(features):
            # âš ï¸ EviMRå‡è®¾æ¯ä¸ªh_iæ˜¯ç‹¬ç«‹è§†å›¾
            alpha_i = evidence_head_i(h_i)
        
        # DSèåˆå‡è®¾å„è§†å›¾ç‹¬ç«‹
        alpha_fused = DS_combine([Î±1, Î±2, Î±3, Î±4])
```

**é—®é¢˜**: 
1. EviMRå‡è®¾å„åˆ†è¾¨ç‡è§†å›¾**ç‹¬ç«‹**
2. GNNå·²ç»**æ··åˆ**äº†å®ƒä»¬
3. ç ´åäº†è¯æ®ç†è®ºçš„ç‹¬ç«‹æ€§å‡è®¾

---

## ğŸ¨ å›¾ç¤ºï¼šæ¶æ„å†²çª

### MedGNNï¼ˆæ­£å¸¸ï¼‰

```
Resolution 1 â”€â”
Resolution 2 â”€â”¤
Resolution 3 â”€â”¤â”€â†’ GNN â†’ Mean â†’ Single Feature â†’ Linear â†’ Output âœ…
Resolution 4 â”€â”˜        â†‘ èåˆç›®çš„æ˜ç¡®
```

### MERIT with GNNï¼ˆå†²çªï¼‰

```
Resolution 1 â”€â”                    â”Œâ”€â†’ Î±1 â”
Resolution 2 â”€â”¤                    â”œâ”€â†’ Î±2 â”œâ”€â†’ DS Fusion âŒ
Resolution 3 â”€â”¤â”€â†’ GNN â†’ [h1',h2',h3',h4'] â”œâ”€â†’ Î±3 â”‚    â†‘
Resolution 4 â”€â”˜        â†‘            â””â”€â†’ Î±4 â”˜  å†²çªï¼
                    æ··åˆäº†                    â†“
                                      å‡è®¾ç‹¬ç«‹
```

**å†²çªç‚¹**:
- GNN: "æˆ‘è¦èåˆæ‰€æœ‰åˆ†è¾¨ç‡"
- EviMR: "æˆ‘è¦å„åˆ†è¾¨ç‡ç‹¬ç«‹ç”Ÿæˆevidence"
- ç»“æœ: ä¸¤è€…ç›®æ ‡ç›¸åï¼

---

## ğŸ“ æ•°å­¦åˆ†æ

### ETMCè¯æ®ç†è®ºå‡è®¾

Dempster-Shaferèåˆå…¬å¼è¦æ±‚ï¼š

```
å‰æ: Î±1, Î±2, Î±3, Î±4 æ¥è‡ªç‹¬ç«‹è§†å›¾
DSèåˆ: Î±_fused = DS(Î±1, Î±2, Î±3, Î±4)

ç‹¬ç«‹æ€§: I(Î±_i; Î±_j) = 0  (i â‰  j)
```

### GNNç ´åç‹¬ç«‹æ€§

```python
# GNNçš„å›¾å·ç§¯æ“ä½œ
h_i' = GCN(h_i, adjacency_matrix)
     = Î£_j w_ij * h_j  # åŒ…å«äº†å…¶ä»–åˆ†è¾¨ç‡çš„ä¿¡æ¯

# å¯¼è‡´
I(Î±_i; Î±_j) > 0  # ä¸å†ç‹¬ç«‹ï¼
```

**ç»“æœ**: DSèåˆçš„æ•°å­¦å‡è®¾è¢«ç ´å

---

## ğŸ” å®éªŒè§‚å¯Ÿ

### è®­ç»ƒè¡Œä¸ºå·®å¼‚

| æŒ‡æ ‡ | æ— GNN | æœ‰GNN |
|------|-------|-------|
| æ”¶æ•›é€Ÿåº¦ | ç¨³å®š | ä¸ç¨³å®š |
| æ–¹å·® | 2.46% | 4.31% |
| æœ€å¥½seed | 81.27% (seed 49) | 81.69% (seed 48) |
| æœ€å·®seed | ~72% | ~69% |

**è§‚å¯Ÿ**: GNNå¯¼è‡´è®­ç»ƒæ›´ä¸ç¨³å®š

### ä¸ªåˆ«seedåˆ†æ

**Seed 48** (è®­ç»ƒæœ€é•¿300ç§’):
- æœ‰GNN: 81.69% â† æœ€å¥½çš„
- ä½†å¹³å‡: 75.33%

è¯´æ˜ï¼š
- æœ‰äº›seedèƒ½workï¼Œä½†å¤§éƒ¨åˆ†ä¸è¡Œ
- GNNéœ€è¦**ç‰¹å®šçš„åˆå§‹åŒ–**æ‰èƒ½ä¸ç ´åç‰¹å¾
- ä¸ç¨³å®šæ€§å¤ªé«˜

---

## âœ… ç»“è®ºä¸å»ºè®®

### 1. ç†è®ºä¸Šï¼šæ¶æ„ä¸å…¼å®¹

```
MedGNN = Transformer + GNN + Mean + Linear
         â†‘ èåˆå¯¼å‘

MERIT  = Transformer + EviMR + DSèåˆ
         â†‘ ç‹¬ç«‹æ€§å¯¼å‘

æ·»åŠ GNN = ç ´åç‹¬ç«‹æ€§ = è¿èƒŒè¯æ®ç†è®ºå‡è®¾
```

### 2. å®éªŒä¸Šï¼šæ€§èƒ½ä¸‹é™

- ğŸ”´ å‡†ç¡®ç‡ä¸‹é™ 2.67%
- ğŸ”´ ä¸ç¨³å®šæ€§å¢åŠ 
- ğŸ”´ è®­ç»ƒæˆæœ¬æ›´é«˜

### 3. æ¨èæ¶æ„

```python
MERITæœ€ä½³æ¶æ„:

Input â†’ Multi-Resolution Data
      â†“
      Frequency Embedding
      â†“
      Difference Attention
      â†“
      Transformer Encoder (ä¿æŒå„åˆ†è¾¨ç‡ç‹¬ç«‹)
      â†“
      [è·³è¿‡GNN] â† å…³é”®ï¼
      â†“
      EviMR (è¯æ®èåˆ)
      - æ¯åˆ†è¾¨ç‡ç‹¬ç«‹ç”Ÿæˆevidence
      - DSç»„åˆç‹¬ç«‹çš„evidence
      â†“
      Output
```

### 4. ä¸MedGNNçš„å·®å¼‚

| æ–¹é¢ | MedGNN | MERIT |
|------|--------|-------|
| **ç‰¹å¾æå–** | âœ… ç›¸åŒ | âœ… ç›¸åŒ |
| **GNN** | âœ… ä½¿ç”¨ | âŒ **ä¸ä½¿ç”¨** |
| **èåˆæ–¹å¼** | ç®€å•Mean | è¯æ®ç†è®º |
| **ä¼˜åŠ¿** | æ•è·ä¾èµ– | é‡åŒ–ä¸ç¡®å®šæ€§ |

**MERITçš„åˆ›æ–°ä¸æ˜¯GNNï¼Œè€Œæ˜¯è¯æ®èåˆï¼**

---

## ğŸ¯ åç»­ä¼˜åŒ–æ–¹å‘

### ä¸ç”¨GNNï¼Œå¦‚ä½•è¶…è¿‡MedGNNï¼Ÿ

å½“å‰: MERIT (æ— GNN) = 78.00% vs MedGNN = 82.60%
å·®è·: -4.6%

**å¯ä¼˜åŒ–çš„æ–¹å‘**:

1. **è¶…å‚æ•°è°ƒä¼˜** (æœ€æœ‰å¸Œæœ›)
   - `lambda_pseudo_loss`: 0.3 â†’ 0.4-0.5
   - `annealing_epoch`: 50 â†’ 30-40
   - `learning_rate`: 1e-4 â†’ 8e-5 æˆ– 1.2e-4

2. **è¯æ®æ¿€æ´»å‡½æ•°**
   - `softplus` â†’ `exp` æˆ– `relu`
   
3. **Pseudo-viewè®¾è®¡**
   - å½“å‰ç®€å•concat
   - å¯å°è¯•attention-basedèåˆ

4. **æ­£åˆ™åŒ–**
   - `weight_decay`: 0 â†’ 1e-5
   - `evidence_dropout`: 0 â†’ 0.1

5. **æŸå¤±æƒé‡**
   - å½“å‰: fuse=1.0, view=1.0, pseudo=0.3
   - å°è¯•: fuse=1.5, view=0.8, pseudo=0.4

---

## ğŸ“ ç»™å®¡ç¨¿äººçš„è¯´æ˜

å¦‚æœè®ºæ–‡ä¸­è§£é‡Šä¸ºä»€ä¹ˆä¸ç”¨GNN:

> **Why MERIT does not use GNN:**
> 
> While MedGNN employs a multi-resolution GNN to fuse features across different temporal scales, MERIT adopts a fundamentally different fusion strategy based on Dempster-Shafer evidence theory. 
>
> The DS fusion in MERIT requires **independent evidence** from each resolution to properly model uncertainty and combine beliefs. Applying GNN would violate this independence assumption by mixing information across resolutions before evidence generation, leading to:
>
> 1. **Theoretical conflict**: DS theory assumes independent information sources
> 2. **Empirical degradation**: Our experiments show 2.67% accuracy drop with GNN
> 3. **Architectural redundancy**: GNN fusion and DS fusion serve similar purposes
>
> Therefore, MERIT directly feeds Transformer outputs to the evidential fusion module, preserving the independence required for principled uncertainty quantification.

---

**æ€»ç»“**: GNNè™½ç„¶åœ¨MedGNNä¸­æœ‰æ•ˆï¼Œä½†åœ¨MERITçš„è¯æ®èåˆæ¡†æ¶ä¸‹**äº§ç”Ÿå†²çª**ï¼Œåº”è¯¥**ä¸ä½¿ç”¨**ã€‚

**æœ€ä½³é…ç½®**: Transformer â†’ EviMR (æ— GNN)
**é¢„æœŸæ€§èƒ½**: 78-80% (éœ€è¿›ä¸€æ­¥è¶…å‚æ•°ä¼˜åŒ–ä»¥è¶…è¿‡82.6%)

