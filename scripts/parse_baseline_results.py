#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è§£æbaselineæ¨¡å‹çš„ç»“æœ
ä»è¾“å‡ºæ–‡æœ¬ä¸­æå–æŒ‡æ ‡
"""
import re
import numpy as np
import sys
import os

def parse_results_from_file(filepath):
    """ä»è¾“å‡ºæ–‡ä»¶ä¸­è§£æç»“æœ"""
    if not os.path.exists(filepath):
        return None
    
    with open(filepath, 'r') as f:
        content = f.read()
    
    # åŒ¹é…æ¨¡å¼ï¼šValidation results --- Loss: X, Accuracy: X, Precision: X, Recall: X, F1: X, AUROC: X, AUPRC: X
    pattern = r"(Validation|Test) results --- Loss: ([0-9\.]+), Accuracy: ([0-9\.]+), Precision: ([0-9\.]+), Recall: ([0-9\.]+), F1: ([0-9\.]+), AUROC: ([0-9\.]+), AUPRC: ([0-9\.]+)"
    
    results = {'val': [], 'test': []}
    for match in re.finditer(pattern, content):
        kind, loss, acc, prec, rec, f1, auroc, auprc = match.groups()
        target = 'val' if kind == 'Validation' else 'test'
        results[target].append({
            'loss': float(loss),
            'acc': float(acc),
            'prec': float(prec),
            'rec': float(rec),
            'f1': float(f1),
            'auroc': float(auroc),
            'auprc': float(auprc),
        })
    
    # è®¡ç®—å¹³å‡å’Œæ ‡å‡†å·®
    summary = {}
    for split in ['val', 'test']:
        if results[split]:
            for metric in ['acc', 'prec', 'rec', 'f1', 'auroc']:
                values = [r[metric] for r in results[split]]
                summary[f'{split}_{metric}_mean'] = np.mean(values)
                summary[f'{split}_{metric}_std'] = np.std(values)
        else:
            for metric in ['acc', 'prec', 'rec', 'f1', 'auroc']:
                summary[f'{split}_{metric}_mean'] = 0.0
                summary[f'{split}_{metric}_std'] = 0.0
    
    summary['n_seeds'] = len(results['test'])
    return summary

def format_result(mean, std):
    """æ ¼å¼åŒ–ç»“æœä¸ºç™¾åˆ†æ¯”"""
    return f"{mean*100:.2f}Â±{std*100:.2f}"

def main():
    if len(sys.argv) < 2:
        print("Usage: python parse_baseline_results.py <DATASET>")
        print("Example: python parse_baseline_results.py APAVA")
        sys.exit(1)
    
    dataset = sys.argv[1]
    results_dir = f"results/baselines/{dataset}"
    
    if not os.path.exists(results_dir):
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
        print(f"è¯·å…ˆè¿è¡Œ: bash MERIT/scripts/run_baselines.sh {dataset}")
        sys.exit(1)
    
    print("\n" + "="*80)
    print(f"Baseline Results on {dataset}")
    print("="*80)
    
    # å®šä¹‰baselineæ¨¡å‹
    baselines = [
        ('Medformer', 'medformer_results.txt'),
        ('iTransformer', 'itransformer_results.txt'),
    ]
    
    all_results = []
    
    for model_name, filename in baselines:
        filepath = os.path.join(results_dir, filename)
        res = parse_results_from_file(filepath)
        
        if res and res['n_seeds'] > 0:
            print(f"\nâœ… {model_name}:")
            print(f"   Seeds: {res['n_seeds']}/10")
            print(f"   Test Acc:    {format_result(res['test_acc_mean'], res['test_acc_std'])}")
            print(f"   Test Prec:   {format_result(res['test_prec_mean'], res['test_prec_std'])}")
            print(f"   Test Rec:    {format_result(res['test_rec_mean'], res['test_rec_std'])}")
            print(f"   Test F1:     {format_result(res['test_f1_mean'], res['test_f1_std'])}")
            print(f"   Test AUROC:  {format_result(res['test_auroc_mean'], res['test_auroc_std'])}")
            
            all_results.append({
                'model': model_name,
                **res
            })
        else:
            print(f"\nâš ï¸  {model_name}: æœªæ‰¾åˆ°ç»“æœæˆ–å…¨éƒ¨å¤±è´¥")
    
    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
    if all_results:
        print("\n" + "="*80)
        print("ğŸ“Š Comparison Table")
        print("="*80)
        print(f"\n{'Model':<15} {'Accuracy':<18} {'Precision':<18} {'Recall':<18} {'F1 Score':<18} {'AUROC':<18}")
        print("-"*115)
        
        for res in all_results:
            acc = format_result(res['test_acc_mean'], res['test_acc_std'])
            prec = format_result(res['test_prec_mean'], res['test_prec_std'])
            rec = format_result(res['test_rec_mean'], res['test_rec_std'])
            f1 = format_result(res['test_f1_mean'], res['test_f1_std'])
            auroc = format_result(res['test_auroc_mean'], res['test_auroc_std'])
            
            print(f"{res['model']:<15} {acc:<18} {prec:<18} {rec:<18} {f1:<18} {auroc:<18}")
        
        # LaTeXæ ¼å¼
        print("\n" + "="*80)
        print("ğŸ“ LaTeX Format")
        print("="*80)
        print()
        for res in all_results:
            acc = format_result(res['test_acc_mean'], res['test_acc_std'])
            prec = format_result(res['test_prec_mean'], res['test_prec_std'])
            rec = format_result(res['test_rec_mean'], res['test_rec_std'])
            f1 = format_result(res['test_f1_mean'], res['test_f1_std'])
            auroc = format_result(res['test_auroc_mean'], res['test_auroc_std'])
            
            print(f"{res['model']} & {acc} & {prec} & {rec} & {f1} & {auroc} \\\\")
    
    print("\n" + "="*80)
    print("ğŸ’¡ æç¤º:")
    print("  - MedGNNçš„ç»“æœå¯ä»¥ä»è®ºæ–‡ä¸­ç›´æ¥å¼•ç”¨")
    print("  - åœ¨APAVAä¸Š: MedGNNè¾¾åˆ°82.60%Â±0.35%")
    print("  - ä½ çš„MERITç»“æœ: è¿è¡Œ bash MERIT/scripts/run_apava.sh")
    print("="*80 + "\n")

if __name__ == '__main__':
    main()

