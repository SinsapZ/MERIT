#!/usr/bin/env python
"""
æ±‡æ€»æ‰€æœ‰å®éªŒç»“æœï¼Œç”Ÿæˆæ’è¡Œæ¦œ
"""

import os
import glob
import pandas as pd
import numpy as np
from pathlib import Path

def load_all_results(results_dir='results/comprehensive'):
    """åŠ è½½æ‰€æœ‰å®éªŒç»“æœ"""
    csv_files = glob.glob(f'{results_dir}/exp*.csv')
    
    all_results = []
    
    for csv_file in sorted(csv_files):
        try:
            df = pd.read_csv(csv_file)
            
            # è¿‡æ»¤æˆåŠŸçš„è¿è¡Œ
            df_success = df[df['return_code'] == 0]
            
            if len(df_success) == 0:
                continue
            
            # è®¡ç®—ç»Ÿè®¡é‡
            stats = {
                'experiment': Path(csv_file).stem,
                'n_seeds': len(df_success),
                'val_acc_mean': df_success['val_acc'].mean(),
                'val_acc_std': df_success['val_acc'].std(),
                'val_f1_mean': df_success['val_f1'].mean(),
                'val_f1_std': df_success['val_f1'].std(),
                'val_auroc_mean': df_success['val_auroc'].mean(),
                'val_auroc_std': df_success['val_auroc'].std(),
                'test_acc_mean': df_success['test_acc'].mean(),
                'test_acc_std': df_success['test_acc'].std(),
                'test_f1_mean': df_success['test_f1'].mean(),
                'test_f1_std': df_success['test_f1'].std(),
                'test_auroc_mean': df_success['test_auroc'].mean(),
                'test_auroc_std': df_success['test_auroc'].std(),
            }
            
            all_results.append(stats)
            
        except Exception as e:
            print(f"Error loading {csv_file}: {e}")
            continue
    
    return pd.DataFrame(all_results)

def print_rankings(df):
    """æ‰“å°æ’è¡Œæ¦œ"""
    
    print("\n" + "="*80)
    print("å®éªŒç»“æœæ’è¡Œæ¦œ - æŒ‰ Test Accuracy æ’åº")
    print("="*80)
    
    # æŒ‰test_accæ’åº
    df_sorted = df.sort_values('test_acc_mean', ascending=False).reset_index(drop=True)
    
    print(f"\n{'æ’å':<4} {'å®éªŒåç§°':<35} {'Test Acc':<15} {'Test F1':<15} {'Test AUROC':<15}")
    print("-"*80)
    
    for idx, row in df_sorted.iterrows():
        rank = idx + 1
        exp_name = row['experiment'].replace('exp', '').replace('_', ' ')
        test_acc = f"{row['test_acc_mean']:.5f}Â±{row['test_acc_std']:.5f}"
        test_f1 = f"{row['test_f1_mean']:.5f}Â±{row['test_f1_std']:.5f}"
        test_auroc = f"{row['test_auroc_mean']:.5f}Â±{row['test_auroc_std']:.5f}"
        
        print(f"{rank:<4} {exp_name:<35} {test_acc:<15} {test_f1:<15} {test_auroc:<15}")
    
    print("\n" + "="*80)
    print("å®éªŒç»“æœæ’è¡Œæ¦œ - æŒ‰ Test F1 æ’åº")
    print("="*80)
    
    # æŒ‰test_f1æ’åº
    df_sorted = df.sort_values('test_f1_mean', ascending=False).reset_index(drop=True)
    
    print(f"\n{'æ’å':<4} {'å®éªŒåç§°':<35} {'Test F1':<15} {'Test Acc':<15} {'Test AUROC':<15}")
    print("-"*80)
    
    for idx, row in df_sorted.iterrows():
        rank = idx + 1
        exp_name = row['experiment'].replace('exp', '').replace('_', ' ')
        test_f1 = f"{row['test_f1_mean']:.5f}Â±{row['test_f1_std']:.5f}"
        test_acc = f"{row['test_acc_mean']:.5f}Â±{row['test_acc_std']:.5f}"
        test_auroc = f"{row['test_auroc_mean']:.5f}Â±{row['test_auroc_std']:.5f}"
        
        print(f"{rank:<4} {exp_name:<35} {test_f1:<15} {test_acc:<15} {test_auroc:<15}")
    
    print("\n" + "="*80)
    print("æœ€ç¨³å®šçš„é…ç½® - æŒ‰æ ‡å‡†å·®æ’åºï¼ˆè¶Šå°è¶Šå¥½ï¼‰")
    print("="*80)
    
    # æŒ‰stdæ’åº
    df_sorted = df.sort_values('test_acc_std', ascending=True).reset_index(drop=True)
    
    print(f"\n{'æ’å':<4} {'å®éªŒåç§°':<35} {'Std(Acc)':<12} {'Mean(Acc)':<15}")
    print("-"*80)
    
    for idx, row in df_sorted.head(10).iterrows():
        rank = idx + 1
        exp_name = row['experiment'].replace('exp', '').replace('_', ' ')
        std = f"{row['test_acc_std']:.5f}"
        mean = f"{row['test_acc_mean']:.5f}"
        
        print(f"{rank:<4} {exp_name:<35} {std:<12} {mean:<15}")

def main():
    print("æ­£åœ¨åŠ è½½å®éªŒç»“æœ...")
    
    # å°è¯•ä¸¤ä¸ªç›®å½•
    if os.path.exists('results/comprehensive'):
        df = load_all_results('results/comprehensive')
    elif os.path.exists('results'):
        df = load_all_results('results')
    else:
        print("é”™è¯¯: æ‰¾ä¸åˆ°ç»“æœç›®å½•")
        return
    
    if len(df) == 0:
        print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å®éªŒç»“æœ")
        return
    
    print(f"æˆåŠŸåŠ è½½ {len(df)} ä¸ªå®éªŒç»“æœ\n")
    
    # æ‰“å°æ’è¡Œæ¦œ
    print_rankings(df)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_file = 'results/comprehensive_summary.csv'
    df.to_csv(output_file, index=False)
    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # æ‰¾åˆ°æœ€ä½³é…ç½®
    best_idx = df['test_acc_mean'].idxmax()
    best_exp = df.loc[best_idx]
    
    print("\n" + "="*80)
    print("ğŸ† æœ€ä½³é…ç½®")
    print("="*80)
    print(f"å®éªŒ: {best_exp['experiment']}")
    print(f"Test Acc: {best_exp['test_acc_mean']:.5f} Â± {best_exp['test_acc_std']:.5f}")
    print(f"Test F1:  {best_exp['test_f1_mean']:.5f} Â± {best_exp['test_f1_std']:.5f}")
    print(f"Test AUROC: {best_exp['test_auroc_mean']:.5f} Â± {best_exp['test_auroc_std']:.5f}")
    print("="*80)

if __name__ == '__main__':
    main()

